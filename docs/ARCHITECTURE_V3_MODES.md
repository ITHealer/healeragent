# HealerAgent AI Chatbot Architecture - V3 Mode System

## Table of Contents
1. [Current Architecture Analysis](#1-current-architecture-analysis)
2. [Gap Analysis](#2-gap-analysis)
3. [Target Architecture: 3-Mode System](#3-target-architecture)
4. [Detailed Implementation Plan](#4-implementation-plan)
5. [File System Structure](#5-file-system-structure)
6. [SSE Event Protocol](#6-sse-event-protocol)
7. [Migration Strategy](#7-migration-strategy)

---

## 1. Current Architecture Analysis

### 1.1 Two Parallel Pipelines (Problem)

HealerAgent currently has **TWO separate chat pipelines** that evolved independently:

#### Pipeline A: `src/routers/v2/chat.py` + `ChatHandler`
```
POST /chat/complete  (non-streaming)
POST /chat/stream    (streaming)

Flow: 7-Phase Upfront Planning Pipeline
  Phase 1: Context Loading (Core Memory + Summary + History + Working Memory)
  Phase 2: Planning (Classify â†’ Load Tools â†’ Create TaskPlan) [2 LLM calls]
  Phase 3: Memory Search
  Phase 4: Tool Execution (TaskExecutor with retry)
  Phase 5: Context Assembly (NO LLM)
  Phase 6: LLM Response Generation (streaming)
  Phase 7: Post-Processing (background memory updates)

Model: Always uses client-provided model_name
Mode: Has `response_mode` param in ChatRequest but NEVER uses it
Router: ModeRouter exists but is DISCONNECTED from this pipeline
```

#### Pipeline B: `src/routers/v2/chat_assistant.py` + `UnifiedAgent`
```
POST /chat-assistant/chat/v2  (streaming SSE)
POST /chat-assistant/chat/v3  (delegates to v2, enables Finance Guru)

Flow: ChatGPT-style Agentic Loop
  Phase 0: Process images (multimodal)
  Phase 1: Session Start + Working Memory Setup
  Phase 1.5: Context Loading via ContextBuilder
  Phase 1.6: Context Compaction Check
  Phase 2: Intent Classification [1 LLM call]
  Phase 3: Agent Loop (UnifiedAgent sees ALL tools, iterative)
  Phase 4: Done + Charts
  Post: LEARN Hook + SAVE + SUMMARY

Model: Client-provided, agent decides tools iteratively
Mode: `mode` param exists but routes between "normal" vs "deep_research" (legacy)
Router: Uses `ModeRouter` but for normal/deep_research, NOT for Instant/Thinking/Auto
```

### 1.2 Key Components Map

```
src/
â”œâ”€â”€ routers/v2/
â”‚   â”œâ”€â”€ chat.py                          # Pipeline A endpoints
â”‚   â””â”€â”€ chat_assistant.py               # Pipeline B endpoints (v2, v3)
â”‚
â”œâ”€â”€ handlers/v2/
â”‚   â”œâ”€â”€ chat_handler.py                  # Pipeline A: 7-phase ChatHandler
â”‚   â”œâ”€â”€ mode_router.py                   # Legacy mode router (normal/deep_research)
â”‚   â””â”€â”€ normal_mode_chat_handler.py      # Legacy normal mode handler
â”‚
â”œâ”€â”€ agents/
â”‚   â”œâ”€â”€ unified/                         # Pipeline B: ChatGPT-style agent
â”‚   â”‚   â””â”€â”€ unified_agent.py             # UnifiedAgent (agentic loop)
â”‚   â”œâ”€â”€ classification/
â”‚   â”‚   â”œâ”€â”€ unified_classifier.py        # Pipeline A classifier
â”‚   â”‚   â””â”€â”€ intent_classifier.py         # Pipeline B classifier (better)
â”‚   â”œâ”€â”€ planning/
â”‚   â”‚   â””â”€â”€ planning_agent.py            # 3-stage planning (Pipeline A only)
â”‚   â”œâ”€â”€ action/
â”‚   â”‚   â””â”€â”€ task_executor.py             # Task execution (Pipeline A only)
â”‚   â”œâ”€â”€ routing/
â”‚   â”‚   â””â”€â”€ mode_router.py              # NEW ModeRouter (Fast/Auto/Expert) - DISCONNECTED
â”‚   â”œâ”€â”€ streaming/
â”‚   â”‚   â”œâ”€â”€ streaming_chat_handler.py    # StreamingChatHandler (Pipeline A streaming)
â”‚   â”‚   â”œâ”€â”€ stream_events.py             # Stream event types
â”‚   â”‚   â””â”€â”€ agent_tree.py               # Agent tree tracking
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â”œâ”€â”€ core_memory.py               # Core Memory (Persona + Human)
â”‚   â”‚   â”œâ”€â”€ recursive_summary.py         # Recursive Summary Manager
â”‚   â”‚   â”œâ”€â”€ working_memory_integration.py # Working Memory (per-request)
â”‚   â”‚   â””â”€â”€ memory_update_agent.py       # Background memory updates
â”‚   â””â”€â”€ tools/
â”‚       â”œâ”€â”€ registry.py                  # Tool Registry (31 tools)
â”‚       â”œâ”€â”€ tool_loader.py               # Tool loading
â”‚       â”œâ”€â”€ base.py                      # Base tool class
â”‚       â””â”€â”€ {category}/*.py              # 31 atomic tools
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ mode_config.py                   # ModeConfig (Fast/Auto/Expert) - EXISTS but unused
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ streaming_event_service.py       # Pipeline B: SSE event emitter
â”‚   â”œâ”€â”€ context_builder.py               # Pipeline B: ContextBuilder
â”‚   â”œâ”€â”€ context_management_service.py    # Context compaction
â”‚   â”œâ”€â”€ conversation_compactor.py        # Conversation compression
â”‚   â”œâ”€â”€ think_tool_service.py            # Think Tool
â”‚   â”œâ”€â”€ tool_search_service.py           # Tool search/discovery
â”‚   â””â”€â”€ memory_search_service.py         # Memory vector search
â”‚
â””â”€â”€ providers/
    â””â”€â”€ provider_factory.py              # Multi-LLM provider routing
```

### 1.3 What Currently Works Well

1. **UnifiedAgent (Pipeline B)** - ChatGPT-style agentic loop with iterative tool calling
2. **IntentClassifier** - Single LLM call classification with symbol validation
3. **Memory System** - 3-tier (Core + Summary + Working Memory) with cross-turn continuity
4. **Tool System** - 31 atomic tools with registry, schemas, circuit breakers
5. **Streaming Events** - Rich SSE protocol with ThinkingTimeline, tool progress
6. **Context Compaction** - Auto-compress when approaching token limits
7. **LEARN Hook** - Background memory updates post-execution

---

## 2. Gap Analysis

### 2.1 Critical Gaps

| # | Gap | Impact | Severity |
|---|-----|--------|----------|
| 1 | **ModeRouter disconnected** - `mode_router.py` and `mode_config.py` exist but are never integrated into any pipeline | No mode-based routing works | CRITICAL |
| 2 | **No Instant Mode path** - No fast path that skips planning and uses nano model | All queries go through full pipeline (~5-15s) | CRITICAL |
| 3 | **No auto-escalation** - When Instant mode gets insufficient data, no mechanism to escalate to Thinking mode | User gets poor answers for complex queries in Instant mode | HIGH |
| 4 | **Model selection ignores mode** - Client always sends model_name, ModeConfig's per-mode model selection never applies | Wrong model for query complexity | HIGH |
| 5 | **Two pipelines = maintenance burden** - Pipeline A (ChatHandler) and Pipeline B (UnifiedAgent) duplicate context loading, memory, saving | Bug fixes needed in 2 places | MEDIUM |
| 6 | **ThinkingTimeline not mode-aware** - Always shows thinking UI regardless of mode | Instant mode shows unnecessary thinking | MEDIUM |
| 7 | **No mode SSE events** - `mode_selecting` and `mode_selected` events defined in docs but never emitted | Frontend can't show mode selection | MEDIUM |

### 2.2 Architecture Comparison with Industry Leaders

| Feature | ChatGPT 5.2 | Claude AI | HealerAgent Current | HealerAgent Target |
|---------|-------------|-----------|---------------------|-------------------|
| Mode selection | Instant/Thinking/Auto | Extended Thinking toggle | N/A (mode exists but disconnected) | Instant/Thinking/Auto |
| Auto-routing | LLM classifies complexity | Model self-decides budget | ModeRouter exists but unused | LLM semantic classification |
| Model per mode | Different models | Single model, variable thinking | Single model always | gpt-4.1-nano / gpt-4.1 / gpt-4o |
| Thinking display | "Thought for Xs" timeline | Thinking block before response | ThinkingTimeline exists | Mode-aware ThinkingTimeline |
| Escalation | Auto when needed | N/A | None | Instant -> Thinking auto-escalate |
| Tool selection | Agent decides | Agent decides | Agent decides (Pipeline B) or planned (Pipeline A) | Mode-aware: filtered (Instant) or all (Thinking) |
| Streaming | Progressive | Progressive | SSE with events | Enhanced SSE with mode events |

---

## 3. Target Architecture

### 3.1 Unified V3 Pipeline with 3-Mode System

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    POST /chat-assistant/chat/v3                       â”‚
â”‚                                                                       â”‚
â”‚  ChatRequest { query, session_id, response_mode, model_name, ... }   â”‚
â”‚                                                                       â”‚
â”‚  response_mode: "instant" | "thinking" | "auto" (default)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Phase 0: Setup & Context Loading                   â”‚
â”‚                                                                       â”‚
â”‚  1. Session Setup + Working Memory                                    â”‚
â”‚  2. ContextBuilder: Core Memory + Summary + History + WM Symbols      â”‚
â”‚  3. Context Compaction (if needed)                                    â”‚
â”‚  4. Process Images (if multimodal)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                                      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Phase 1: Mode Resolution                           â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ User chose       â”‚    â”‚ User chose       â”‚                        â”‚
â”‚  â”‚ "instant"?       â”‚    â”‚ "thinking"?      â”‚                        â”‚
â”‚  â”‚ â†’ Use INSTANT    â”‚    â”‚ â†’ Use THINKING   â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚ User chose "auto" (default)?             â”‚                        â”‚
â”‚  â”‚                                           â”‚                        â”‚
â”‚  â”‚  1. Quick heuristics (< 1ms):             â”‚                        â”‚
â”‚  â”‚     - Short query + 0-1 symbols â†’ INSTANT â”‚                        â”‚
â”‚  â”‚     - Multiple symbols â†’ THINKING         â”‚                        â”‚
â”‚  â”‚     - Context continuity â†’ inherit prev   â”‚                        â”‚
â”‚  â”‚                                           â”‚                        â”‚
â”‚  â”‚  2. If undecided â†’ LLM classify (200ms): â”‚                        â”‚
â”‚  â”‚     - gpt-4.1-nano with JSON output       â”‚                        â”‚
â”‚  â”‚     - Returns: simple/complex + confidenceâ”‚                        â”‚
â”‚  â”‚     - Cache result (5min TTL)             â”‚                        â”‚
â”‚  â”‚                                           â”‚                        â”‚
â”‚  â”‚  3. Map: simple â†’ INSTANT, complex â†’ THINKING â”‚                   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚                                                                       â”‚
â”‚  SSE: emit mode_selecting â†’ mode_selected                            â”‚
â”‚  Result: ModeConfig { model, max_turns, features, ... }              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                      â”‚
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚                       â”‚
                          â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      INSTANT MODE PATH         â”‚ â”‚       THINKING MODE PATH           â”‚
â”‚                                â”‚ â”‚                                    â”‚
â”‚  Model: gpt-4.1-nano          â”‚ â”‚  Model: gpt-4.1 or gpt-4o         â”‚
â”‚  Target: < 3 seconds          â”‚ â”‚  Target: 10-30 seconds             â”‚
â”‚  Max turns: 2                  â”‚ â”‚  Max turns: 6                      â”‚
â”‚  Thinking display: OFF         â”‚ â”‚  Thinking display: ON              â”‚
â”‚  Web search: OFF               â”‚ â”‚  Web search: ON (if needed)        â”‚
â”‚  Tool search mode: OFF         â”‚ â”‚  Tool search mode: ON              â”‚
â”‚  Finance Guru: OFF             â”‚ â”‚  Finance Guru: ON                  â”‚
â”‚                                â”‚ â”‚                                    â”‚
â”‚  Phase 2: IntentClassifier     â”‚ â”‚  Phase 2: IntentClassifier         â”‚
â”‚    (1 LLM call, nano model)    â”‚ â”‚    (1 LLM call, standard model)    â”‚
â”‚                                â”‚ â”‚                                    â”‚
â”‚  Phase 3: Agent Loop           â”‚ â”‚  Phase 3: Planning Agent           â”‚
â”‚    (1-2 tool calls max)        â”‚ â”‚    (Create detailed TaskPlan)      â”‚
â”‚    (No evaluation loop)        â”‚ â”‚                                    â”‚
â”‚    (Filtered tools: 8-10)      â”‚ â”‚  Phase 4: Evaluation Loop          â”‚
â”‚                                â”‚ â”‚    (Execute â†’ Evaluate â†’ Retry)    â”‚
â”‚  Phase 4: Stream Response      â”‚ â”‚    (Max 3 iterations)              â”‚
â”‚    (Direct, no thinking UI)    â”‚ â”‚    (All 31+ tools available)       â”‚
â”‚                                â”‚ â”‚                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚  Phase 5: Stream Response          â”‚
â”‚  â”‚ ESCALATION CHECK         â”‚  â”‚ â”‚    (With ThinkingTimeline UI)      â”‚
â”‚  â”‚                          â”‚  â”‚ â”‚                                    â”‚
â”‚  â”‚ After Phase 3, check:    â”‚  â”‚ â”‚  Phase 6: Post-Processing          â”‚
â”‚  â”‚ - Tool errors > 50%?     â”‚  â”‚ â”‚    (LEARN + SAVE + SUMMARY)        â”‚
â”‚  â”‚ - No data returned?      â”‚  â”‚ â”‚                                    â”‚
â”‚  â”‚ - Confidence < 0.5?      â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚  â”‚                          â”‚  â”‚
â”‚  â”‚ If YES â†’ escalate to     â”‚  â”‚
â”‚  â”‚ THINKING mode silently   â”‚  â”‚
â”‚  â”‚ (user sees better result)â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                â”‚
â”‚  Phase 5: Post-Processing      â”‚
â”‚    (LEARN + SAVE + SUMMARY)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Mode Configurations (Target)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Feature          â”‚ INSTANT MODE âš¡       â”‚ THINKING MODE ğŸ§      â”‚ AUTO MODE ğŸ”„         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model (primary)  â”‚ gpt-4.1-nano         â”‚ gpt-4.1              â”‚ Depends on classify  â”‚
â”‚ Model (fallback) â”‚ gpt-4o-mini          â”‚ gpt-4o               â”‚ gpt-4.1-nano         â”‚
â”‚ Target latency   â”‚ < 3 seconds          â”‚ 10-30 seconds        â”‚ 3-30 seconds         â”‚
â”‚ Max agent turns  â”‚ 2                    â”‚ 6                    â”‚ 2-6                  â”‚
â”‚ Tool count       â”‚ 8-10 (filtered)      â”‚ 31+ (all)            â”‚ Depends on classify  â”‚
â”‚ Web search       â”‚ OFF                  â”‚ ON                   â”‚ ON if complex        â”‚
â”‚ Thinking display â”‚ OFF                  â”‚ ON (timeline)        â”‚ Depends on classify  â”‚
â”‚ Think Tool       â”‚ OFF                  â”‚ ON                   â”‚ ON if complex        â”‚
â”‚ Finance Guru     â”‚ OFF                  â”‚ ON                   â”‚ ON if complex        â”‚
â”‚ Tool search mode â”‚ OFF (direct tools)   â”‚ ON (dynamic discover)â”‚ Depends on classify  â”‚
â”‚ Evaluation loop  â”‚ NO                   â”‚ YES (max 3)          â”‚ Depends on classify  â”‚
â”‚ Escalation       â”‚ â†’ THINKING if needed â”‚ N/A                  â”‚ Auto                 â”‚
â”‚ System prompt    â”‚ Condensed (1.5K tok) â”‚ Full (4K tok)        â”‚ Depends              â”‚
â”‚ Classifier model â”‚ Same nano model      â”‚ Not needed           â”‚ gpt-4.1-nano (200ms) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Instant Mode - Detailed Flow

```
User: "GiÃ¡ AAPL?" + mode=instant

1. [0ms] Setup + Context Loading (parallel)
   â”œâ”€â”€ Load Working Memory symbols
   â”œâ”€â”€ Load Core Memory
   â””â”€â”€ Load last 5 messages (not 10)

2. [50ms] Mode Resolution: INSTANT (explicit or auto-classified)
   â””â”€â”€ SSE: mode_selected { mode: "instant", model: "gpt-4.1-nano" }

3. [100ms] IntentClassifier (nano model, 1 LLM call)
   â”œâ”€â”€ symbols: ["AAPL"]
   â”œâ”€â”€ requires_tools: true
   â”œâ”€â”€ complexity: direct
   â””â”€â”€ SSE: classified { symbols: ["AAPL"], complexity: "direct" }

4. [200ms] Agent Loop (max 2 turns)
   â”œâ”€â”€ Turn 1: getStockPrice(symbol="AAPL")
   â”œâ”€â”€ SSE: tool_calls â†’ tool_results
   â””â”€â”€ Turn 2: Generate response (nano model, streaming)

5. [2000ms] Stream Response
   â””â”€â”€ SSE: content chunks (NO thinking timeline)

6. [2500ms] Post-Processing (background)
   â””â”€â”€ SAVE conversation + LEARN memory update

Total: ~2.5 seconds
LLM calls: 2 (classify + response)
Tool calls: 1
```

### 3.4 Thinking Mode - Detailed Flow

```
User: "PhÃ¢n tÃ­ch toÃ n diá»‡n AAPL - technical, fundamental, risk, so sÃ¡nh thá»‹ trÆ°á»ng" + mode=thinking

1. [0ms] Setup + Context Loading
   â”œâ”€â”€ Load full context (10 messages + summary + core memory)
   â””â”€â”€ Context compaction if needed

2. [50ms] Mode Resolution: THINKING
   â””â”€â”€ SSE: mode_selected { mode: "thinking", model: "gpt-4.1" }

3. [200ms] IntentClassifier (standard model)
   â”œâ”€â”€ symbols: ["AAPL"]
   â”œâ”€â”€ requires_tools: true
   â”œâ”€â”€ complexity: agent_loop
   â””â”€â”€ SSE: classified + thinking_timeline starts

4. [400ms] Planning Agent creates detailed TaskPlan
   â”œâ”€â”€ Task 1: getStockPrice + getTechnicalIndicators
   â”œâ”€â”€ Task 2: getFinancialRatios + getGrowthMetrics
   â”œâ”€â”€ Task 3: assessRisk + getVolumeProfile
   â”œâ”€â”€ Task 4: getMarketIndices + getSectorPerformance
   â””â”€â”€ SSE: planning_progress â†’ planning_complete

5. [2000ms] Execution Loop (with evaluation)
   â”œâ”€â”€ Execute all tasks (parallel where possible)
   â”œâ”€â”€ SSE: tool_start â†’ tool_complete (for each tool)
   â”œâ”€â”€ Evaluation: "Data sufficient?" (1 LLM call)
   â”œâ”€â”€ If insufficient: execute additional tools (up to 3 iterations)
   â””â”€â”€ SSE: thinking_timeline steps

6. [5000ms] Stream Response (gpt-4.1, full system prompt)
   â”œâ”€â”€ SSE: thinking_summary { duration: "Thought for 8s", steps: [...] }
   â””â”€â”€ SSE: content chunks

7. [15000ms] Post-Processing
   â””â”€â”€ SAVE + LEARN + SUMMARY

Total: ~15 seconds
LLM calls: 3-5 (classify + plan + evaluate + response)
Tool calls: 6-12
```

### 3.5 Auto Mode - Detailed Flow

```
User: "NVDA" + mode=auto (default)

1. [0ms] Setup + Context Loading

2. [50ms] Mode Resolution: AUTO
   â”œâ”€â”€ SSE: mode_selecting { method: "auto" }
   â”œâ”€â”€ Quick heuristics: short query (4 chars) + 1 symbol â†’ INSTANT
   â”œâ”€â”€ Result: INSTANT (confidence: 0.85)
   â””â”€â”€ SSE: mode_selected { mode: "instant", reason: "very_short_query" }

3. [100ms] â†’ Follow INSTANT path

---

User: "So sÃ¡nh NVDA vÃ  AMD vá» máº·t fundamental" + mode=auto

1. [0ms] Setup + Context Loading

2. [50ms] Mode Resolution: AUTO
   â”œâ”€â”€ SSE: mode_selecting { method: "auto" }
   â”œâ”€â”€ Quick heuristics: 2 symbols detected â†’ THINKING
   â”œâ”€â”€ Result: THINKING (confidence: 0.90)
   â””â”€â”€ SSE: mode_selected { mode: "thinking", reason: "multi_symbol_detected" }

3. [200ms] â†’ Follow THINKING path
```

### 3.6 Escalation Mechanism

```
Instant Mode executes...
Agent Loop completes with:
  - 2/3 tool calls failed
  - No meaningful data returned
  - OR response confidence < 0.5

Escalation triggered:
  1. Log: "[ESCALATION] Instant â†’ Thinking | reason: insufficient_data"
  2. SSE: mode_escalated { from: "instant", to: "thinking", reason: "..." }
  3. Re-execute with THINKING mode config:
     - Switch model: gpt-4.1-nano â†’ gpt-4.1
     - Expand tools: 8 â†’ 31
     - Increase turns: 2 â†’ 6
     - Enable ThinkingTimeline
  4. User sees: seamless better response (slightly longer wait)
```

---

## 4. Implementation Plan

### Phase 1: Mode Infrastructure (Foundation)

#### Task 1.1: Rename Response Modes to Match User Terms
**File:** `src/config/mode_config.py`
**Changes:**
- Rename `ResponseMode.FAST` â†’ `ResponseMode.INSTANT`
- Rename `ResponseMode.EXPERT` â†’ `ResponseMode.THINKING`
- Keep `ResponseMode.AUTO`
- Update `INSTANT_MODE_CONFIG`, `THINKING_MODE_CONFIG`, `AUTO_MODE_CONFIG`
- Update models: gpt-4o-mini â†’ gpt-4.1-nano (instant), gpt-4o â†’ gpt-4.1 (thinking)
- Add `evaluation_loop` field to ModeConfig
- Add `escalation_enabled` field to ModeConfig
- Add `max_history_messages` field (5 for instant, 10 for thinking)

#### Task 1.2: Integrate ModeRouter into V3 Pipeline
**File:** `src/agents/routing/mode_router.py`
**Changes:**
- Update `QueryComplexity` enum: SIMPLE/COMPLEX
- Update classification prompt to support Instant/Thinking terminology
- Update heuristics for better accuracy
- Add escalation detection method: `should_escalate(tool_results, error_rate, confidence)`
- Add method: `get_mode_config(mode_result) â†’ ModeConfig`

#### Task 1.3: Create Mode-Aware Configuration Resolver
**New File:** `src/services/mode_resolver_service.py`
**Purpose:** Single entry point for resolving mode â†’ config, including:
- Explicit mode selection (user chose instant/thinking)
- Auto classification (LLM + heuristics)
- Escalation decisions
- Override model/features per mode

```python
class ModeResolverService:
    async def resolve_mode(
        query: str,
        user_mode: str,  # "instant" | "thinking" | "auto"
        context: ModeContext
    ) -> ResolvedMode:
        """Returns: mode, config, model_name, features"""

    def should_escalate(
        mode: str,
        tool_results: List,
        error_count: int,
        confidence: float
    ) -> EscalationDecision:
        """Decide if instant should escalate to thinking"""
```

### Phase 2: V3 Pipeline Refactor

#### Task 2.1: Create Unified V3 Chat Handler
**New File:** `src/handlers/v3/chat_handler.py`
**Purpose:** Single handler that unifies both pipelines with mode awareness

```python
class V3ChatHandler:
    """
    Unified V3 Chat Handler with 3-Mode System.

    Replaces: ChatHandler (Pipeline A) + stream_chat_v2 (Pipeline B)

    Flow:
    1. Setup + Context Loading
    2. Mode Resolution
    3. Intent Classification (mode-aware model)
    4. Agent Execution (mode-aware config)
    5. Escalation Check (instant only)
    6. Response Streaming (mode-aware display)
    7. Post-Processing
    """
```

**Key Design Decisions:**
- Base on Pipeline B (UnifiedAgent) as it's more mature
- Add mode-aware wrapping layer on top
- Use IntentClassifier (not PlanningAgent) for both modes
- For Thinking mode: add evaluation loop after agent execution
- For Instant mode: limit agent turns and disable features

#### Task 2.2: Create V3 Route
**New File:** `src/routers/v3/chat.py`
**Changes:**
- New router with prefix `/v3`
- Endpoint: `POST /v3/chat/stream`
- Updated `ChatRequest` with `response_mode` field properly typed
- Delegate to V3ChatHandler

#### Task 2.3: Update ChatRequest Schema
**File:** `src/routers/v3/chat.py` (new)
**Changes:**
```python
class V3ChatRequest(BaseModel):
    query: str
    session_id: Optional[str]
    response_mode: Literal["instant", "thinking", "auto"] = "auto"
    model_name: Optional[str] = None  # None = mode decides
    provider_type: str = "openai"
    # ... other fields
```

**Key:** `model_name` is now Optional. If None, the mode config decides the model. If provided, it overrides (for advanced users).

### Phase 3: Instant Mode Implementation

#### Task 3.1: Instant Mode Agent Configuration
**File:** `src/handlers/v3/chat_handler.py`
**Changes:**
- When mode=INSTANT:
  - Use gpt-4.1-nano for classification AND response
  - Limit UnifiedAgent to max_turns=2
  - Disable tool_search_mode (use direct tool names)
  - Disable web_search, think_tool, finance_guru
  - Reduce conversation history to last 5 messages
  - Use condensed system prompt
  - Skip ThinkingTimeline emission

#### Task 3.2: Tool Filtering for Instant Mode
**New File:** `src/services/tool_filter_service.py`
**Purpose:** Filter tool catalog based on mode config
```python
class ToolFilterService:
    def filter_for_instant(
        all_tools: List[ToolSchema],
        categories: List[str] = ["price", "technical", "fundamentals", "news"]
    ) -> List[ToolSchema]:
        """Return only essential tools for instant mode"""
        # Max 10 tools
        # Prioritize: GetStockPrice, GetTechnicalIndicators, GetStockNews, etc.
```

#### Task 3.3: Escalation Logic
**File:** `src/handlers/v3/chat_handler.py`
**Changes:**
- After instant mode agent completes, evaluate:
  - `error_rate = failed_tools / total_tools`
  - `has_meaningful_data = any(result.success for result in results)`
  - `response_confidence` from agent output
- If escalation triggered:
  - Re-run with thinking mode config
  - Emit `mode_escalated` SSE event
  - Use accumulated tool results (don't re-fetch successful ones)

### Phase 4: Thinking Mode Implementation

#### Task 4.1: Evaluation Loop Service
**New File:** `src/services/evaluation_service.py`
**Purpose:** "Data sufficient?" check after tool execution

```python
class EvaluationService:
    async def evaluate_completeness(
        query: str,
        intent: IntentResult,
        tool_results: List[Dict],
        iteration: int,
        max_iterations: int = 3
    ) -> EvaluationResult:
        """
        Evaluate if gathered data is sufficient.
        Returns: sufficient (bool), missing_data (list), suggested_tools (list)
        Uses 1 LLM call with nano model for speed.
        """
```

#### Task 4.2: Thinking Mode Extended Pipeline
**File:** `src/handlers/v3/chat_handler.py`
**Changes:**
- When mode=THINKING:
  - Use gpt-4.1 for classification and response
  - Enable ALL features (web search, think tool, finance guru, tool search)
  - After agent loop completes, run EvaluationService
  - If evaluation says insufficient: execute suggested tools, re-evaluate
  - Max 3 evaluation iterations
  - Enable full ThinkingTimeline with detailed steps
  - Use full system prompt with examples

#### Task 4.3: Enhanced ThinkingTimeline for Thinking Mode
**File:** `src/agents/streaming/stream_events.py`
**Changes:**
- Add `mode_selecting` event type
- Add `mode_selected` event type
- Add `mode_escalated` event type
- Add `evaluation_start` / `evaluation_complete` event types
- Make ThinkingTimeline mode-aware: skip emissions for instant mode

### Phase 5: SSE Protocol Enhancement

#### Task 5.1: New SSE Events for Mode System
**File:** `src/services/streaming_event_service.py`
**Changes:**
```python
# New event emitters
def emit_mode_selecting(method: str) -> str
def emit_mode_selected(mode: str, reason: str, model: str, confidence: float) -> str
def emit_mode_escalated(from_mode: str, to_mode: str, reason: str) -> str
def emit_evaluation_start(iteration: int) -> str
def emit_evaluation_complete(sufficient: bool, missing: list) -> str
```

#### Task 5.2: Mode-Aware Event Filtering
**File:** `src/handlers/v3/chat_handler.py`
**Changes:**
- In instant mode: suppress thinking_timeline events, evaluation events
- In thinking mode: emit all events
- In auto mode: emit mode_selecting before classification, then follow resolved mode

### Phase 6: Integration & Testing

#### Task 6.1: Register V3 Router
**File:** `src/main.py`
**Changes:**
- Import and include v3 router
- Keep v2 routes for backward compatibility

#### Task 6.2: Update Provider Factory for Mode Models
**File:** `src/providers/provider_factory.py`
**Changes:**
- Add method: `get_model_for_mode(mode_config: ModeConfig) -> str`
- Support gpt-4.1-nano, gpt-4.1, gpt-4o model routing

#### Task 6.3: Integration Tests
**New Files:** `tests/v3/`
- test_mode_resolution.py
- test_instant_mode.py
- test_thinking_mode.py
- test_auto_mode.py
- test_escalation.py

---

## 5. File System Structure

### New Files to Create

```
src/
â”œâ”€â”€ routers/
â”‚   â””â”€â”€ v3/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ chat.py                     # V3 chat endpoint with response_mode
â”‚
â”œâ”€â”€ handlers/
â”‚   â””â”€â”€ v3/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ chat_handler.py             # Unified V3 ChatHandler (3-mode)
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ mode_resolver_service.py        # Mode resolution + escalation decisions
â”‚   â”œâ”€â”€ tool_filter_service.py          # Filter tools based on mode
â”‚   â””â”€â”€ evaluation_service.py           # "Data sufficient?" evaluation loop
â”‚
â””â”€â”€ config/
    â””â”€â”€ mode_config.py                  # Updated: INSTANT/THINKING/AUTO configs
```

### Files to Modify

```
src/
â”œâ”€â”€ config/
â”‚   â””â”€â”€ mode_config.py                  # Rename FASTâ†’INSTANT, EXPERTâ†’THINKING
â”‚
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ routing/
â”‚       â””â”€â”€ mode_router.py              # Update for instant/thinking terminology
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ streaming_event_service.py      # Add mode_selecting/selected/escalated events
â”‚
â”œâ”€â”€ agents/
â”‚   â””â”€â”€ streaming/
â”‚       â””â”€â”€ stream_events.py            # Add new event types
â”‚
â”œâ”€â”€ providers/
â”‚   â””â”€â”€ provider_factory.py             # Add get_model_for_mode()
â”‚
â””â”€â”€ main.py                             # Register v3 router
```

### Files NOT Modified (Backward Compatible)

```
src/routers/v2/chat.py                  # Keep as-is (Pipeline A)
src/routers/v2/chat_assistant.py        # Keep as-is (Pipeline B: v2, v3 legacy)
src/handlers/v2/chat_handler.py         # Keep as-is (Pipeline A handler)
src/agents/unified/unified_agent.py     # Keep as-is (reused by V3)
src/agents/classification/intent_classifier.py  # Keep as-is (reused by V3)
```

---

## 6. SSE Event Protocol (V3)

### Complete Event Flow - Auto Mode

```
1. SSE: session_start    { version: "v3", session_id: "..." }
2. SSE: mode_selecting   { method: "auto", query_length: 45 }
3. SSE: mode_selected    { mode: "instant", reason: "single_symbol_simple", model: "gpt-4.1-nano", confidence: 0.85 }
4. SSE: classifying      { }
5. SSE: classified       { query_type: "stock_specific", symbols: ["AAPL"], ... }
6. SSE: turn_start       { turn: 1, max_turns: 2 }
7. SSE: tool_calls       { tools: [{ name: "getStockPrice", arguments: { symbol: "AAPL" } }] }
8. SSE: tool_results     { results: [{ name: "getStockPrice", success: true, ... }] }
9. SSE: content          { content: "AAPL is currently..." }
10. SSE: content         { content: "trading at $198.50..." }
... more content chunks
11. SSE: thinking_summary { total_duration_ms: 2100, steps: [...] }
12. SSE: done            { total_turns: 1, total_tool_calls: 1, total_time_ms: 2500 }
13. [DONE]
```

### Complete Event Flow - Thinking Mode

```
1. SSE: session_start    { version: "v3" }
2. SSE: mode_selected    { mode: "thinking", reason: "explicit_user_selection", model: "gpt-4.1" }
3. SSE: classifying      { }
4. SSE: thinking_step    { phase: "classification", action: "Analyzing query..." }
5. SSE: classified       { ... }
6. SSE: thinking_step    { phase: "tool_selection", action: "Agent starting with 31 tools" }
7. SSE: turn_start       { turn: 1 }
8. SSE: tool_calls       { tools: [{ name: "getStockPrice" }, { name: "getTechnicalIndicators" }] }
9. SSE: thinking_step    { phase: "tool_execution", action: "Calling 2 tools", details: "..." }
10. SSE: tool_results    { results: [...] }
11. SSE: turn_start      { turn: 2 }
12. SSE: tool_calls      { tools: [{ name: "getFinancialRatios" }, { name: "assessRisk" }] }
13. SSE: tool_results    { results: [...] }
14. SSE: evaluation      { iteration: 1, sufficient: true }
15. SSE: content         { content: "..." }
... more content
16. SSE: thinking_summary { total_duration_ms: 12000, steps: [...] }
17. SSE: sources         { citations: [...] }  // if web search used
18. SSE: done            { total_turns: 3, total_tool_calls: 5, mode: "thinking" }
19. [DONE]
```

### Escalation Event Flow

```
... (instant mode events)
7. SSE: tool_results     { results: [{ success: false }, { success: false }] }
8. SSE: mode_escalated   { from: "instant", to: "thinking", reason: "tool_error_rate_high" }
9. SSE: thinking_step    { phase: "escalation", action: "Upgrading to deeper analysis..." }
... (continues with thinking mode events)
```

---

## 7. Migration Strategy

### Phase 1: Non-Breaking (Add V3 alongside V2)
- Create `/v3/chat/stream` endpoint
- Keep all V2 endpoints unchanged
- V3 reuses V2 components (UnifiedAgent, IntentClassifier, etc.)
- Frontend can opt-in to V3 by changing endpoint

### Phase 2: Feature Parity
- V3 supports all V2 features (images, reply_to, ui_context, etc.)
- V3 adds mode system on top
- V2 remains as fallback

### Phase 3: Deprecation (Future)
- Mark V2 endpoints as deprecated
- Frontend migrates to V3
- Remove V2 after full migration

### Backward Compatibility Rules
1. `response_mode="auto"` is default - behaves like V2 for simple queries
2. If `model_name` is explicitly provided, it overrides mode-based model selection
3. All existing SSE events remain unchanged - new events are additions
4. UnifiedAgent, IntentClassifier, tools - all reused, not rewritten
5. Memory system (Core + Summary + Working) - unchanged

---

## Summary: Implementation Priority Order

```
Priority 1 (Foundation):
  1.1 Update mode_config.py (rename + new fields)
  1.2 Update mode_router.py (integrate)
  1.3 Create mode_resolver_service.py

Priority 2 (Core Pipeline):
  2.1 Create V3 ChatHandler
  2.2 Create V3 Route
  2.3 Update ChatRequest schema

Priority 3 (Instant Mode):
  3.1 Instant mode agent config
  3.2 Tool filtering
  3.3 Escalation logic

Priority 4 (Thinking Mode):
  4.1 Evaluation service
  4.2 Extended pipeline
  4.3 Enhanced ThinkingTimeline

Priority 5 (SSE Protocol):
  5.1 New SSE events
  5.2 Mode-aware filtering

Priority 6 (Integration):
  6.1 Register router
  6.2 Provider factory update
  6.3 Tests
```

Each task is independent enough to be implemented and tested individually. The dependency chain is:
`Phase 1 â†’ Phase 2 â†’ Phase 3 & 4 (parallel) â†’ Phase 5 â†’ Phase 6`
