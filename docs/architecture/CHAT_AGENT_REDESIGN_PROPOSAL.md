# Chat Agent Architecture Redesign Proposal

**Date:** 2026-01-22
**Status:** Proposal
**Author:** AI Architecture Review

---

## Executive Summary

Äá» xuáº¥t tÃ¡i cáº¥u trÃºc AI chatbot flow Ä‘á»ƒ:
1. Há»— trá»£ **Response Modes** (Fast/Auto/Expert) nhÆ° cÃ¡c AI chatbot hiá»‡n Ä‘áº¡i
2. **Loáº¡i bá» Intent Classification** khi dÃ¹ng large models (redundant)
3. **ÄÆ¡n giáº£n hÃ³a architecture** Ä‘á»ƒ dá»… scale vÃ  maintain
4. **Unified provider abstraction** Ä‘á»ƒ trÃ¡nh bugs khi switch models

---

## 1. Current Architecture Analysis

### 1.1 Existing Flow (V2 Endpoint)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     CURRENT FLOW (7 PHASES)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

User Request
    â”‚
    â–¼
[Phase 1] Context Building (~100ms)
    â”œâ”€ Load Core Memory
    â”œâ”€ Load Conversation Summary
    â”œâ”€ Load Recent History (10 messages)
    â””â”€ Load Working Memory Symbols
    â”‚
    â–¼
[Phase 2] Intent Classification (~300-800ms) âš ï¸ BOTTLENECK
    â”œâ”€ 1 LLM call (gpt-4.1-mini)
    â”œâ”€ Symbol extraction & normalization
    â”œâ”€ Complexity determination
    â”œâ”€ Market type detection
    â””â”€ Analysis type classification
    â”‚
    â–¼
[Phase 3] Agent Execution (~3-30s)
    â”œâ”€ 1-6 LLM calls with tools
    â”œâ”€ Tool execution (parallel)
    â””â”€ Streaming response
    â”‚
    â–¼
[Phase 4-7] Post-processing (~500ms)
    â”œâ”€ Save conversation
    â”œâ”€ Update working memory
    â”œâ”€ Create summaries
    â””â”€ Chart resolution
```

### 1.2 Problems Identified

| Problem | Impact | Severity |
|---------|--------|----------|
| Intent Classification always runs | +300-800ms latency, +cost | HIGH |
| No user control over response speed | Poor UX | HIGH |
| Complex 7-phase pipeline | Hard to maintain | MEDIUM |
| GPT/Gemini separation caused bugs | Reliability issues | HIGH |
| Memory update blocks response | Slower perceived speed | MEDIUM |

### 1.3 Intent Classification - Redundancy Analysis

#### When Classification IS Needed:
```
Small Models (gpt-4o-mini, gemini-flash):
â”œâ”€ Cannot reliably select correct tools from 30+ options
â”œâ”€ May misunderstand complex queries
â”œâ”€ Benefit from pre-filtered tool set
â””â”€ Need symbol normalization hints
```

#### When Classification is NOT Needed:
```
Large Models (gpt-4o, gemini-2.5-pro, claude-sonnet):
â”œâ”€ Excellent tool selection from full catalog
â”œâ”€ Understand context and intent natively
â”œâ”€ Can normalize "Google" â†’ "GOOGL" themselves
â””â”€ Self-determine complexity and steps needed
```

**Conclusion:** Intent Classification is **REDUNDANT** for large models.

---

## 2. Proposed Architecture

### 2.1 Response Modes Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     RESPONSE MODE SELECTION                      â”‚
â”‚                                                                  â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                â”‚
â”‚   â”‚  AUTO   â”‚      â”‚  FAST   â”‚      â”‚ EXPERT  â”‚                â”‚
â”‚   â”‚   ğŸ”„    â”‚      â”‚   âš¡    â”‚      â”‚   ğŸ§     â”‚                â”‚
â”‚   â”‚ Default â”‚      â”‚ Speed   â”‚      â”‚ Quality â”‚                â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚
â”‚                                                                  â”‚
â”‚   Adapts to        Respond          Think further               â”‚
â”‚   each query       quicker          to explore                  â”‚
â”‚                    to act sooner    deeper                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Mode Specifications

#### âš¡ FAST Mode
```yaml
Purpose: "Respond quicker to act sooner"
Target Latency: 3-8 seconds

Configuration:
  model: gpt-4o-mini / gemini-2.0-flash
  use_classifier: true  # Guide small model
  max_turns: 2
  tool_set: filtered_top_5
  web_search: false
  system_prompt: condensed

Use Cases:
  - "GiÃ¡ AAPL?"
  - "PE ratio MSFT?"
  - "RSI lÃ  gÃ¬?"
  - Simple lookups
  - Quick definitions
```

#### ğŸ§  EXPERT Mode
```yaml
Purpose: "Think further to explore deeper"
Target Latency: 15-60 seconds

Configuration:
  model: gpt-4o / gemini-2.5-pro / claude-sonnet
  use_classifier: false  # Model decides everything
  max_turns: 6
  tool_set: all_tools
  web_search: true
  system_prompt: full_with_examples

Use Cases:
  - "So sÃ¡nh toÃ n diá»‡n NVDA vÃ  AMD"
  - "PhÃ¢n tÃ­ch ká»¹ thuáº­t + Ä‘á»‹nh giÃ¡ GOOGL"
  - "Chiáº¿n lÆ°á»£c Ä‘áº§u tÆ° AI stocks 2026"
  - Multi-step analysis
  - Research tasks
```

#### ğŸ”„ AUTO Mode (Default)
```yaml
Purpose: "Adapts models to each query"

Selection Logic (NO LLM CALL - Pure Heuristics):

  def select_mode(query, context):
      # Rule 1: Short simple queries â†’ FAST
      if len(query) < 50 and no_complex_indicators(query):
          return FAST

      # Rule 2: Complex keywords â†’ EXPERT
      complex_keywords = [
          "so sÃ¡nh", "phÃ¢n tÃ­ch chi tiáº¿t", "toÃ n diá»‡n",
          "nghiÃªn cá»©u", "chiáº¿n lÆ°á»£c", "Ä‘Ã¡nh giÃ¡ sÃ¢u",
          "compare", "analyze", "comprehensive", "research"
      ]
      if any(kw in query.lower() for kw in complex_keywords):
          return EXPERT

      # Rule 3: Multi-symbol queries â†’ EXPERT
      if count_symbols(query) >= 3:
          return EXPERT

      # Rule 4: Context continuity
      if context.previous_mode == EXPERT:
          return EXPERT  # Maintain context

      # Default: FAST for speed
      return FAST
```

### 2.3 New Simplified Flow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    NEW FLOW (3 PHASES)                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                        User Request
                    + response_mode param
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [PHASE 1] MODE ROUTER (No LLM, ~10ms)                           â”‚
â”‚                                                                  â”‚
â”‚   Input: query, response_mode, context                          â”‚
â”‚   Output: effective_mode, model_config                          â”‚
â”‚                                                                  â”‚
â”‚   if response_mode == "auto":                                   â”‚
â”‚       effective_mode = heuristic_select(query, context)         â”‚
â”‚   else:                                                         â”‚
â”‚       effective_mode = response_mode                            â”‚
â”‚                                                                  â”‚
â”‚   model_config = MODE_CONFIGS[effective_mode]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â–¼                              â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     FAST PATH        â”‚      â”‚    EXPERT PATH       â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚      â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚                      â”‚      â”‚                      â”‚
â”‚ [Optional Classifier]â”‚      â”‚ [Skip Classifier]    â”‚
â”‚ Quick intent check   â”‚      â”‚ Model self-decides   â”‚
â”‚ ~300ms if needed     â”‚      â”‚                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                              â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [PHASE 2] UNIFIED AGENT EXECUTION                                â”‚
â”‚                                                                  â”‚
â”‚   Same flow for all modes, different configs:                   â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚   â”‚ Config from MODE_CONFIGS:                                â”‚   â”‚
â”‚   â”‚   - model_name: str                                      â”‚   â”‚
â”‚   â”‚   - provider_type: str                                   â”‚   â”‚
â”‚   â”‚   - max_turns: int                                       â”‚   â”‚
â”‚   â”‚   - tool_set: list[str] | "all"                         â”‚   â”‚
â”‚   â”‚   - enable_web_search: bool                              â”‚   â”‚
â”‚   â”‚   - system_prompt_version: str                           â”‚   â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                                  â”‚
â”‚   Agent Loop:                                                    â”‚
â”‚   for turn in range(max_turns):                                 â”‚
â”‚       response = await llm.chat(messages, tools)                â”‚
â”‚       if no_tool_calls(response):                               â”‚
â”‚           break  # Generate final response                      â”‚
â”‚       results = await execute_tools_parallel(tool_calls)        â”‚
â”‚       messages.append(results)                                  â”‚
â”‚                                                                  â”‚
â”‚   yield streaming_response                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [PHASE 3] ASYNC POST-PROCESSING (Non-blocking)                   â”‚
â”‚                                                                  â”‚
â”‚   # Response already streaming to user                          â”‚
â”‚   # These run in background:                                    â”‚
â”‚                                                                  â”‚
â”‚   asyncio.create_task(save_conversation(...))                   â”‚
â”‚   asyncio.create_task(update_memory(...))                       â”‚
â”‚   asyncio.create_task(create_summary_if_needed(...))            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Provider Abstraction (Unified Multi-Model)

### 3.1 Problem: Why GPT/Gemini Separation Failed

```
Previous Approach (FAILED):
â”œâ”€ Separate code paths for GPT vs Gemini
â”œâ”€ Different message formats not properly converted
â”œâ”€ Gemini thought_signature not preserved across turns
â”œâ”€ Tool call format differences caused silent failures
â””â”€ Debugging nightmare - which path had the bug?
```

### 3.2 Solution: Single Flow + Provider Interface

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    PROVIDER ABSTRACTION LAYER                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   UNIFIED AGENT   â”‚
                    â”‚                   â”‚
                    â”‚ Uses abstract     â”‚
                    â”‚ LLMProvider       â”‚
                    â”‚ interface only    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           LLMProvider Interface          â”‚
        â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
        â”‚ + format_messages(msgs) â†’ provider_fmt  â”‚
        â”‚ + format_tools(tools) â†’ provider_fmt    â”‚
        â”‚ + call(messages, tools) â†’ response      â”‚
        â”‚ + parse_response(resp) â†’ unified_fmt    â”‚
        â”‚ + stream(messages, tools) â†’ events      â”‚
        â”‚ + handle_special_features(...)          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OpenAIProvider  â”‚  â”‚ GeminiProvider  â”‚  â”‚ OpenRouterProv  â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Native format   â”‚  â”‚ Convert msgs    â”‚  â”‚ Multi-model     â”‚
â”‚ Native tools    â”‚  â”‚ Convert tools   â”‚  â”‚ gateway         â”‚
â”‚                 â”‚  â”‚ Handle thought_ â”‚  â”‚                 â”‚
â”‚                 â”‚  â”‚ signature       â”‚  â”‚                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.3 Unified Response Format

```python
@dataclass
class UnifiedLLMResponse:
    """All providers return this format."""

    content: Optional[str]  # Text response
    tool_calls: List[ToolCall]  # Normalized tool calls
    finish_reason: str  # "stop" | "tool_calls" | "length"

    # Provider-specific (preserved for multi-turn)
    raw_response: Any  # Original response object
    provider_metadata: Dict  # thought_signature, etc.

    # Usage tracking
    input_tokens: int
    output_tokens: int


@dataclass
class ToolCall:
    """Normalized tool call format."""

    id: str
    name: str
    arguments: Dict[str, Any]

    # Provider-specific (for response)
    raw_call: Any  # Original format for sending back
```

---

## 4. Configuration-Based Mode System

### 4.1 Mode Configurations

```python
MODE_CONFIGS = {
    "fast": ModeConfig(
        display_name="Fast",
        description="Respond quicker to act sooner",
        icon="âš¡",

        # Model settings
        model_name="gpt-4o-mini",
        provider_type="openai",
        fallback_model="gemini-2.0-flash",
        fallback_provider="gemini",

        # Behavior settings
        use_classifier=True,
        max_turns=2,
        tool_selection="filtered",  # Top 5 relevant
        enable_web_search=False,

        # Prompt settings
        system_prompt_version="condensed",
        include_examples=False,

        # Timeouts
        classifier_timeout_ms=500,
        agent_turn_timeout_ms=10000,
        total_timeout_ms=15000,
    ),

    "expert": ModeConfig(
        display_name="Expert",
        description="Think further to explore deeper",
        icon="ğŸ§ ",

        # Model settings
        model_name="gpt-4o",
        provider_type="openai",
        fallback_model="gemini-2.5-pro",
        fallback_provider="gemini",

        # Behavior settings
        use_classifier=False,  # Model decides everything
        max_turns=6,
        tool_selection="all",
        enable_web_search=True,

        # Prompt settings
        system_prompt_version="full",
        include_examples=True,

        # Timeouts
        classifier_timeout_ms=0,  # Not used
        agent_turn_timeout_ms=30000,
        total_timeout_ms=120000,
    ),

    "auto": ModeConfig(
        display_name="Auto",
        description="Adapts models to each query",
        icon="ğŸ”„",

        # Determined at runtime by heuristics
        # Falls back to "fast" or "expert" config
    ),
}
```

### 4.2 API Request Changes

```python
class ChatRequest(BaseModel):
    """Updated request model."""

    # Existing fields
    query: str
    session_id: Optional[str]
    user_id: str

    # NEW: Response mode selection
    response_mode: Literal["auto", "fast", "expert"] = "auto"

    # Optional: Override specific settings
    model_override: Optional[str] = None  # Force specific model
    provider_override: Optional[str] = None  # Force provider
```

### 4.3 SSE Events (Updated)

```javascript
// NEW: Mode selection event
{
    type: "mode_selected",
    mode: "fast",  // or "expert"
    reason: "auto_heuristic: short_query",  // Why this mode
    config: { model: "gpt-4o-mini", max_turns: 2 }
}

// Existing events continue...
{ type: "turn_start", turn: 1, max_turns: 2 }
{ type: "tool_calls", tools: [...] }
{ type: "content", content: "...", is_final: false }
{ type: "done", mode_used: "fast", total_time_ms: 4500 }
```

---

## 5. Implementation Plan

### Phase 1: Foundation (Week 1)
```
â–¡ Add response_mode to ChatRequest
â–¡ Create MODE_CONFIGS dictionary
â–¡ Implement heuristic mode selector for AUTO
â–¡ Add mode_selected SSE event
â–¡ Update frontend to show mode indicator
```

### Phase 2: Fast Mode (Week 2)
```
â–¡ Create condensed system prompt
â–¡ Implement tool filtering logic
â–¡ Set up 2-turn limit
â–¡ Test latency targets (3-8s)
â–¡ A/B test against current flow
```

### Phase 3: Expert Mode (Week 3)
```
â–¡ Remove classifier for expert mode
â–¡ Enable full tool set
â–¡ Configure 6-turn limit
â–¡ Enable web search by default
â–¡ Test quality vs current flow
```

### Phase 4: Provider Abstraction (Week 4)
```
â–¡ Define LLMProvider interface
â–¡ Refactor OpenAI provider
â–¡ Refactor Gemini provider (thought_signature)
â–¡ Add fallback mechanism
â–¡ Integration testing
```

### Phase 5: Cleanup (Week 5)
```
â–¡ Remove legacy /chat endpoint (if ready)
â–¡ Move memory updates to background
â–¡ Performance optimization
â–¡ Documentation
â–¡ Monitoring dashboards
```

---

## 6. Metrics & Success Criteria

### 6.1 Latency Targets

| Mode | P50 | P90 | P99 |
|------|-----|-----|-----|
| Fast | 4s | 6s | 10s |
| Expert | 20s | 40s | 60s |
| Current | 8s | 15s | 30s |

### 6.2 Quality Metrics

```
Fast Mode:
  - Answer accuracy: â‰¥ 90% (simple queries)
  - Tool selection accuracy: â‰¥ 85%

Expert Mode:
  - Answer completeness: â‰¥ 95%
  - Multi-step success rate: â‰¥ 90%
  - User satisfaction: â‰¥ 4.5/5
```

### 6.3 Cost Efficiency

```
Expected savings from skipping classifier in Expert mode:
  - ~$0.002 per request (gpt-4.1-mini call)
  - At 10K requests/day = ~$600/month saved

Expected increase from larger models:
  - Expert mode costs ~3x Fast mode
  - But only used for ~30% of queries (AUTO selection)
```

---

## 7. Risk Assessment

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| Fast mode quality drops | Medium | High | A/B test, fallback to classifier |
| Expert mode too slow | Low | Medium | Timeout limits, streaming |
| AUTO heuristics wrong | Medium | Medium | Monitor, tune thresholds |
| Provider abstraction bugs | Medium | High | Comprehensive tests |
| User confusion with modes | Low | Low | Good UX, tooltips |

---

## 8. Decision Points

### 8.1 Cáº§n quyáº¿t Ä‘á»‹nh trÆ°á»›c khi implement:

1. **Default mode cho user má»›i?**
   - Option A: AUTO (adaptive)
   - Option B: FAST (speed first)
   - Recommendation: **AUTO**

2. **Cho phÃ©p user override model khÃ´ng?**
   - Option A: Yes, advanced settings
   - Option B: No, hide complexity
   - Recommendation: **Yes, but hidden in settings**

3. **Giá»¯ legacy /chat endpoint?**
   - Option A: Deprecate immediately
   - Option B: Parallel run 1 month
   - Recommendation: **Parallel run, then deprecate**

4. **Memory update strategy?**
   - Option A: Background async (non-blocking)
   - Option B: Sync after response (blocking)
   - Recommendation: **Background async**

---

## Appendix A: Heuristic Mode Selection Logic

```python
class AutoModeSelector:
    """Pure heuristic mode selection - NO LLM CALLS."""

    COMPLEX_KEYWORDS = {
        "vi": [
            "so sÃ¡nh", "phÃ¢n tÃ­ch chi tiáº¿t", "toÃ n diá»‡n",
            "nghiÃªn cá»©u", "chiáº¿n lÆ°á»£c", "Ä‘Ã¡nh giÃ¡ sÃ¢u",
            "giáº£i thÃ­ch", "táº¡i sao", "nhÆ° tháº¿ nÃ o"
        ],
        "en": [
            "compare", "analyze", "comprehensive",
            "research", "strategy", "deep dive",
            "explain", "why", "how does"
        ]
    }

    SIMPLE_PATTERNS = [
        r"^giÃ¡\s+\w+\??$",  # "giÃ¡ AAPL?"
        r"^price\s+\w+\??$",  # "price AAPL?"
        r"^\w+\s+lÃ  gÃ¬\??$",  # "RSI lÃ  gÃ¬?"
        r"^what is\s+\w+\??$",  # "what is RSI?"
    ]

    def select(
        self,
        query: str,
        context: ChatContext
    ) -> Literal["fast", "expert"]:
        """Select mode based on query characteristics."""

        query_lower = query.lower().strip()

        # Rule 1: Very short simple queries â†’ FAST
        if len(query) < 30:
            for pattern in self.SIMPLE_PATTERNS:
                if re.match(pattern, query_lower):
                    return "fast"

        # Rule 2: Complex keywords â†’ EXPERT
        for lang, keywords in self.COMPLEX_KEYWORDS.items():
            if any(kw in query_lower for kw in keywords):
                return "expert"

        # Rule 3: Multiple symbols (â‰¥3) â†’ EXPERT
        symbols = extract_symbols(query)
        if len(symbols) >= 3:
            return "expert"

        # Rule 4: Long query (>150 chars) â†’ EXPERT
        if len(query) > 150:
            return "expert"

        # Rule 5: Previous turn was expert â†’ Continue EXPERT
        if context.previous_mode == "expert":
            return "expert"

        # Rule 6: Contains question requiring reasoning â†’ EXPERT
        reasoning_indicators = [
            "táº¡i sao", "why", "nÃªn", "should",
            "cÃ³ nÃªn", "liá»‡u", "whether"
        ]
        if any(ind in query_lower for ind in reasoning_indicators):
            return "expert"

        # Default: FAST for speed
        return "fast"
```

---

## Appendix B: Migration Checklist

```
Pre-migration:
â–¡ Backup current config
â–¡ Set up feature flags
â–¡ Create rollback plan
â–¡ Notify stakeholders

During migration:
â–¡ Deploy with feature flag OFF
â–¡ Enable for internal testing
â–¡ Enable for 10% traffic
â–¡ Monitor metrics
â–¡ Enable for 50% traffic
â–¡ Monitor 24h
â–¡ Enable for 100%

Post-migration:
â–¡ Remove feature flags
â–¡ Update documentation
â–¡ Archive legacy code
â–¡ Performance report
```

---

**Document Version:** 1.0
**Last Updated:** 2026-01-22
**Next Review:** After Phase 1 implementation
